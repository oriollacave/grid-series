{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EDF Renewables Wind Flow Model Competition Speedup Calculation\n",
    "*by Florian Roscheck, Data Scientist, Wind Resource Assessment Group, EDF Renewables, 2019-11-06*\n",
    "\n",
    "This notebook explains how we, the EDF Renewables Wind Resource Assessment Group, calculate mast speedups in the \n",
    "Wind Flow Model Competition. It provides the context, the Python function that is used to calculate speedups, as well \n",
    "as a ready-to-use-and-play-with example. It is meant to enable you, the contestant, to provide the most appropriate data\n",
    "for your entry in the Wind Flow Model Competition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook (and the speedup calculation) you should have the following Python libraries and versions \n",
    "installed:\n",
    "```\n",
    "python==3.7.3\n",
    "numpy==1.16.4\n",
    "scipy==1.3.1\n",
    "faker==2.0.3\n",
    "pandas==0.24.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context: Which Speedups are Calculated?\n",
    "\n",
    "This section explains which information we provide and which speedups you should calculate.\n",
    "\n",
    "Every site in the competition has multiple masts. For every site, we calculate the speedups between all mast \n",
    "combinations. A mast consists of a speed and a direction time series. We supply measured time series data for only one \n",
    "mast per site (the \"reference\" mast). You are expected to use your wind flow model to predict the wind speed speedups \n",
    "relative to that reference mast at all other masts.\n",
    "\n",
    "We provide mast location, elevation, measurement height, and exposure information (where applicable) for all masts at \n",
    "the site. At every mast, there is only one measurement height that you need to predict. That height is usually between\n",
    "50 and 80 m and specified in the site documents we provide to you. When we score your entry, we only score it against\n",
    "our measurements at that height.\n",
    " \n",
    "For scoring your model, we weigh errors by direction. This helps you and us understand how well your model captures the\n",
    "wind flow in the most dominant wind directions. For this reason, we supply a directional time series for the reference\n",
    "mast and expect you to bin your speedup data by direction. The next section explains in detail how this binning works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculating Speedups by Total Least Squares Regression\n",
    "<a id='tls_regression_explanation'></a>\n",
    "\n",
    "When we score your model, we compare your submitted directional speedups to the ones that we obtained from our measured\n",
    "data. This section outlines how we obtain those speedups.\n",
    "\n",
    "We want to calculate the directional speedups between two masts. One of them is the *initiation* mast, \n",
    "identified by $I$. The other one is the *target* mast, identified by $T$. Every mast has a speed time series $v$ and a\n",
    "direction time series $\\theta$. That means we have the following time series available: $v_I$, $v_T$, $\\theta_I$, \n",
    "$\\theta_T$. We then perform the following steps:\n",
    "\n",
    "1. Get concurrent data $v_{I,c}$, $v_{T,c}$, and $\\theta_{T,c}$ of $v_I$, $v_T$, and $\\theta_T$. We use measured \n",
    "    10-min time series.\n",
    "    \n",
    "2. Bin $v_{I,c}$ and $v_{T,c}$ by target mast direction $\\theta_{T,c}$. There are 24 uniform directional bins $b$ and \n",
    "    the first bin is centered at $0^\\circ$. So, for example, the first bin $b_0$ would be \n",
    "    $352.5^\\circ \\leq \\theta_T < 7.5^\\circ$. Binned data is denoted by $v_{I,c,b}$ and $v_{T,c,b}$, respectively.\n",
    "    \n",
    "3. For every bin $b$ of $\\theta_{T,c}$, by means of total least squares regression, fit a model of the form \n",
    "    $y_b = m_b * x_b$ to the speed data, with $x_b$ being $v_{I,c,b}$ and $y_b$ being $v_{T,c,b}$. The slope \n",
    "    of the model, $m_b$, is the *directional speedup*. \n",
    "  \n",
    "On our side, we use Scipy's Orthogonal Distance Regression package and the following code for the total least squares\n",
    "regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell will not return anything, it will just make sure that the fit_tls function is loaded and can be accessed in this notebook\n",
    "import numpy as np\n",
    "from scipy.odr import odrpack\n",
    "\n",
    "def fit_tls(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Fit total least squares model to data\n",
    "\n",
    "    This function fits a total least squares (also known as orthogonal distance regression) model to 2-dimensional data.\n",
    "    The model has the form `y = m * x`, where m is the \"slope\". (It has an intercept at y==0).\n",
    "\n",
    "    See Also:\n",
    "        At its core, the function uses scipy's `Orthogonal Distance Regression`_ module\n",
    "\n",
    "    .. _Orthogonal Distance Regression:\n",
    "        https://docs.scipy.org/doc/scipy/reference/odr.html\n",
    "\n",
    "    Args:\n",
    "        x: 1-dimensional Numpy array\n",
    "        y: 1-dimensional Numpy array of the same size as `x`\n",
    "\n",
    "    Returns:\n",
    "        The slope of the fitted model\n",
    "\n",
    "    \"\"\"\n",
    "    odr_data = odrpack.RealData(x, y)\n",
    "    model = odrpack.Model(lambda beta, x_: beta[0] * x_)\n",
    "    odr_container = odrpack.ODR(odr_data, model, beta0=[1.0])\n",
    "    slope = odr_container.run().beta[0]\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup Calculation Example\n",
    "\n",
    "This section walks you through an actual speedup calculation (albeit with \"fake\" data). We have validated the code in \n",
    "this section with actual measured data, so you can be certain that this is exactly what we are doing behind the scenes \n",
    "to come up with our directional speedups that we compare your submitted speedups against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate Fake Time Series\n",
    "\n",
    "For this example, we first generate some fake speed and direction time series for initiation and target mast. While no\n",
    "particular emphasis is placed on making the time series realistic, \n",
    "\n",
    "For convenience, we use the [Faker](https://github.com/joke2k/faker) Python package to quickly come up with some time \n",
    "series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize and seed Faker\n",
    "from faker import Faker\n",
    "fake = Faker('en_US')\n",
    "fake.seed(42) # Do not change this seed, we rely on it for verifying that you get the same results that we get.\n",
    "\n",
    "# Initialize and seed numpy\n",
    "import numpy as np\n",
    "np.random.seed(42) # Do not change this seed, we rely on it for verifying that you get the same results that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define fake time series generators\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_spd(weibull_k: float, weibull_c: float, drr: float) -> float:\n",
    "    if np.random.uniform() < drr:\n",
    "        return weibull_c*np.random.weibull(weibull_k)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def generate_dir(peak: float, spread_std: float, drr: float) -> float:\n",
    "    if np.random.uniform() < drr:\n",
    "        value = np.random.normal(peak, spread_std)\n",
    "        return np.mod(value, 360.0)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Set distributions of values for Faker package to sample from\n",
    "mast_initiation_spd_distrib = lambda _: generate_spd(weibull_k=2.25, weibull_c=8.25, drr=0.85)\n",
    "mast_initiation_dir_distrib = lambda _: generate_dir(peak=90.0, spread_std=60.0, drr=0.85)\n",
    "mast_target_spd_distrib = lambda _: generate_spd(weibull_k=2.0, weibull_c=8.0, drr=0.85)\n",
    "mast_target_dir_distrib = lambda _: generate_dir(peak=80.0, spread_std=70.0, drr=0.85)\n",
    "\n",
    "start_date = datetime.strptime('2015-01-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2019-01-01', '%Y-%m-%d')\n",
    "time_interval = timedelta(minutes=10)\n",
    "\n",
    "mast_intiation_spd = fake.time_series(start_date, end_date, time_interval, mast_initiation_spd_distrib)\n",
    "mast_intiation_dir = fake.time_series(start_date, end_date, time_interval, mast_initiation_dir_distrib)\n",
    "mast_target_spd = fake.time_series(start_date, end_date, time_interval, mast_target_spd_distrib)\n",
    "mast_target_dir = fake.time_series(start_date, end_date, time_interval, mast_target_dir_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate fake time series\n",
    "# (Running this cell will take a while)\n",
    "import pandas as pd\n",
    "\n",
    "mast_initiation_spd_series = pd.DataFrame(mast_intiation_spd).set_index(0)[1]\n",
    "print('Generated \"mast_initiation_spd_series\".')\n",
    "\n",
    "mast_initiation_dir_series = pd.DataFrame(mast_intiation_dir).set_index(0)[1]\n",
    "print('Generated \"mast_initiation_dir_series\".')\n",
    "\n",
    "mast_target_spd_series = pd.DataFrame(mast_target_spd).set_index(0)[1]\n",
    "print('Generated \"mast_target_spd_series\".')\n",
    "\n",
    "mast_target_dir_series = pd.DataFrame(mast_target_dir).set_index(0)[1]\n",
    "print('Generated \"mast_target_dir_series\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Concurrent Data\n",
    "\n",
    "Just as explained [above](#tls_regression_explanation) in step 1, we first need to obtain concurrent data. Note that we\n",
    "do not need the `mast_initiation_dir_series` for any of the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put all relevant time series into one Pandas DataFrame\n",
    "all_masts = pd.concat([mast_initiation_spd_series, mast_target_spd_series, mast_target_dir_series], axis=1)\n",
    "all_masts.columns = ['SPD_Initiation', 'SPD_Target', 'DIR_Target']\n",
    "all_masts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filter concurrent data\n",
    "print('No. of samples before filtering by concurrent timesteps: {}'.format(all_masts.shape[0]))\n",
    "all_masts_conc = all_masts.dropna().copy()\n",
    "print('No. of samples after filtering by concurrent timesteps: {}'.format(all_masts_conc.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bin Concurrent Data\n",
    "\n",
    "Now we bin the concurrent speed data by the concurrent target direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_of_bins = 24\n",
    "bin_width = 360.0/no_of_bins\n",
    "all_masts_conc.loc[:, 'Bin'] = np.floor_divide(np.mod(all_masts_conc['DIR_Target'] + (bin_width / 2.0), 360), bin_width) + 1\n",
    "all_masts_conc.loc[:, 'Bin'] = all_masts_conc.loc[:, 'Bin'].astype(int)\n",
    "all_masts_conc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "speedups = {}\n",
    "\n",
    "for bin_no in np.unique(all_masts_conc['Bin'].values):\n",
    "    x_in_bin = all_masts_conc[all_masts_conc['Bin'] == bin_no]['SPD_Initiation'].values\n",
    "    y_in_bin = all_masts_conc[all_masts_conc['Bin'] == bin_no]['SPD_Target'].values\n",
    "    slope = fit_tls(x=x_in_bin, y=y_in_bin)\n",
    "    speedups[bin_no] = slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "speedups = pd.Series(speedups)\n",
    "speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Great**, these are the speedups!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Validate Calculation\n",
    "\n",
    "How do you know that you got exactly the same value that we got? On your path to running this code, some things might \n",
    "have taken a wrong turn. For example: You could have accidentally used the wrong NumPy version, or the computer system\n",
    "that you are working on produces a slightly different result because of its internal configuration.\n",
    "\n",
    "We want to confirm that when you run the code on your system you get the exact same answer that we got when we run it\n",
    "on ours. This will give you certainty that you can calculate speedups the exact same way that we can and help you \n",
    "optimize your entry in the wind flow model competition.\n",
    "\n",
    "Run the following cell to know if your results match ours. Note that for the results to match you need to have all cells\n",
    "executed in order. If you run into trouble, try restarting the kernel and running all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if abs(0.77664505-speedups.prod()) < 1e-7:\n",
    "    print('SUCCESS! You can calculate speedups the exact same way that EDF Renewables can.')\n",
    "else:\n",
    "    print('ERROR! You are not getting the same results as EDF Renewables. Why don\\'t you reach out to Florian to figure'\n",
    "          ' out what to do next?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
